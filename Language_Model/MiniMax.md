## :page_facing_up: MiniMax-01: Scaling Foundation Models with Lightning Attention

### Info

* `publication`: arxiv 2025
* `author`: MiniMax et al.
* `url`: https://arxiv.org/pdf/2501.08313
* `code`: https://github.com/huggingface/transformers/pull/35831

### 1. Abstract & Introduction

ìµœê·¼ Large Language Models (LLMs)ì™€ Vision Language Models (VLMs)ê°€ ë¹ ë¥´ê²Œ ë°œì „í•˜ì˜€ìœ¼ë‚˜ ì´ë“¤ì˜ context ê¸¸ì´ëŠ” 32K~256K í† í°ì— ë¶ˆê³¼í•˜ë‹¤. ì´ëŠ” ì±… ì „ë¶€ë¥¼ contextë¡œ ì“´ë‹¤ë˜ì§€, programming projectì˜ ì „ì²´ ì½”ë“œë¥¼ contextë¡œ ì“°ë˜ê°€, ì—¬ëŸ¬ exampleì„ ì´ìš©í•´ inferenceë¥¼ í•  ê²½ìš° contextê°€ ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤.

context window í™•ì¥ì„ ìœ„í•´ ì—¬ëŸ¬ ë°©ë²•ë“¤ì´ ì œì•ˆë˜ì—ˆì§€ë§Œ transformer êµ¬ì¡°ì— ë‚´ì¬ëœ $O(n^2)$ ê³„ì‚° ë³µì¡ë„ë•Œë¬¸ì— í•œê³„ê°€ ìˆì—ˆë‹¤(context í™•ì¥ì‹œ hardware ëŠ¥ë ¥ë³´ë‹¤ ê³„ì‚° ìš”êµ¬ëŸ‰ì´ í›¨ì”¬ ë§ì•„ì§€ê²Œ ë¨). ì´ì— ë”°ë¼ ì—¬ëŸ¬ attention ë©”ì»¤ë‹ˆì¦˜ë“¤ì´ ì œì•ˆë˜ì—ˆì§€ë§Œ commercial-scale model(Large-scale model) ì ìš©ì— ìˆì–´ì„œëŠ” í•œê³„ê°€ ìˆì—ˆë‹¤.

- ìš°ë¦¬ëŠ” í•œ ì°¨ì› ë” ê¸´ context windowë¥¼ ì œê³µí•˜ë©´ì„œ commercial-scale modelê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ì˜€ë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ìŒ 3ê°€ì§€ ìš”ì†Œë¥¼ ì‹ ì¤‘íˆ ì¡°ì ˆí•˜ì˜€ë‹¤

  - network architexture: lightning attentionê³¼ I/O-aware implementation of a linear attention variantë¥¼ ì‚¬ìš©í•œ Hybrid êµ¬ì¡°

  - data: ë‹¤ì–‘í•œ high-quality corpusì— data cleaning ì‘ì—…, reward-based quality í–¥ìƒ, data mixture balancing ì‘ì—…ì„ ìˆ˜í–‰

  - computation: Mixture of Experts (MoE) êµ¬í˜„, MoE ë‚´ì˜ all-to-all communicationì„ ìœ„í•´ Expert Parallel (EP) ì™€ Expert Tensor Parallel (ETP) ì‚¬ìš©, context windowì˜ ì œí•œì—†ëŠ” í™•ì¥ì„ ìœ„í•´ varlen ring attentionì„ ì„¤ê³„í•˜ì˜€ìœ¼ë©° Linear Attention Sequence Parallelism (LASP)ë¥¼ ê°œì„ ì‹œí‚´

    - ì¶”ê°€ë¡œ lightning attention inferenceì˜ ë§ì¶¤ CUDA kernels setì„ êµ¬í˜„

- **Contributions**

  - í‘œì¤€ academic benchmarksì—ì„œ top-tier closed-source modelì— ë²„ê¸ˆê°€ëŠ” ëª¨ë¸ ê°œë°œí•˜ì˜€ìœ¼ë©°, ì´ ëª¨ë¸ì€ 4M tokenê¹Œì§€ì˜ context inputì„ ì§€ì›í•˜ì—¬ long-context í‰ê°€ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ

  - linear attentionì„ ì´ìš©í•œ ì²« large-scale model êµ¬í˜„í•˜ì˜€ìœ¼ë©°, ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ì™€ engineering ìµœì í™”ì— ëŒ€í•œ ì¢…í•©ì ì¸ ìƒì„¸ ì„¤ëª…ì„ ì œê³µ

  - ë‹¤ì–‘í•œ ëª¨ë¸, ë°ì´í„°ì…‹, í‰ê°€, ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ íƒêµ¬ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ ì ‘ê·¼ ë°©ì‹ê³¼ ì‹¤í—˜ ë°©ë²•ë¡ ì˜ outline ì œê³µ

  - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ê³µê°œí•˜ê³  API ì œê³µ

<br></br>

### 2. Model Architecture

<div align="center"><img src="https://github.com/user-attachments/assets/af08cf68-4fed-4177-89fd-db63cfe37e2e" width="40%"></img></div>   

ì œí•œëœ ë¦¬ì†ŒìŠ¤ì™€ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ë©´ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ **Mixture of Experts(MoE) approachì™€ linear attention** ì„ ì ìš©í–ˆìœ¼ë©°, ì´ë¥¼ ì´ìš©í•´ ìµœëŒ€í•œ traditional softmax functionì„ ëŒ€ì²´í•˜ì˜€ë‹¤.

ì œì•ˆí•˜ëŠ” êµ¬ì¡°ëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ìœ¼ë©°, channel mixer(attention block) ê³¼ feature mixer(MLP block)ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.

- 2ê°€ì§€ channel mixer: lightning attention, softmax attention

  - ë§¤ 7ê°œì˜ linear attention í›„ì— softmax attentionì„ ë‘ì—ˆë‹¤(ì´ë ‡ê²Œ ì´ 80 layersë¡œ êµ¬ì„±).

  - ê° attentionì€ 64 headsë¥¼ ê°€ì§€ë©°, ê° head dimensionì€ 128ì´ë‹¤.

  - softmax attentionì€ Group Query Attention(GQA)ì„ ì ìš©í•˜ì˜€ìœ¼ë©° Group sizeëŠ” 8ì„ ì‚¬ìš©í–ˆë‹¤.

  - Rotary Position Embedding(RoPE)ì´ attention headê°€ ê°€ì§€ëŠ” ì°¨ì› ë°˜ì ˆì— ì ìš©í•˜ì˜€ë‹¤(base frequency set=10,000).
  
  - model hidden size = 6144, ê° layerëŠ” 32 experts(expertsì˜ hidden sizeëŠ” 9216)ë¥¼ í¬í•¨í•˜ë©° top-2 routing ì „ëµ ì‚¬ìš©.

- feature mixer: ì—¬ëŸ¬ feed-forward networksë¥¼ ê°€ì§€ëŠ” MoE

  - MoE block(FFNN block)ì˜ load balancingì„ ìœ„í•´ GShardì—ì„œ ì˜ê°ì„ ë°›ì€ ìƒˆë¡œìš´ load balancing ë°©ë²• ì œì•ˆ (global routerë¼ ë¶€ë¦„)

  - ë˜í•œ, DeepNormì´ ì „ì²´ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í¬í•¨ë˜ì—ˆë‹¤.

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ MiniMax-Text-01ì€ 456B íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©°, 45.9B íŒŒë¼ë¯¸í„°ê°€ ê° í† í° ì²˜ë¦¬ ì‹œ í™œì„±í™” ëœë‹¤.

#### 2-1. Mixture of Experts

MoEëŠ” ì—¬ëŸ¬ FFN expertsë¡œ ì´ë£¨ì–´ì§€ë©°, ê° í† í°ì€ 1ê°œ ì´ìƒì˜ expertsë¡œ ë³´ë‚´ì§„ë‹¤. input token $x_t$ ì™€ ì´ì— ëŒ€í•œ output hidden state $h_t$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.

- $h_t = \sum^E_{i=1} {Softmax}_i (TopK(x_t \cdot W_g)) \cdot {FFN}_i(x_t)$

  - `E`: expertsì˜ ì´ ê°œìˆ˜

  - $W_g$: ê²Œì´íŠ¸(ë¼ìš°í„°) ê°€ì¤‘ì¹˜

  - `FFN`: ië²ˆì§¸ expert

  - `TopK()`: ëª¨ë“  experts ì‚¬ì´ì—ì„œì˜ top-k score, ë‚˜ë¨¸ì§€ scoreëŠ” $-\infty$

MoE training efficiencyë¥¼ ìœ„í•´ token-drop strategyë¥¼ ì±„íƒí•˜ì˜€ë‹¤. ê° expertëŠ” ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ê°œìˆ˜(capacity limit)ê°€ ì§€ì •ë˜ê³  ì´ì— ë„ë‹¬í•˜ë©´ ì´í›„ì˜ í† í°ë“¤ì€ ë²„ë ¤ì§„ë‹¤.

ëª¨ë¸ í¬ê¸°ë¥¼ í‚¤ì› ì„ ë•Œ, routing collapseë¥¼ ê²ªê²Œ ë˜ì—ˆë‹¤. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´, ê°„ë‹¨í•œ global routing strategyë¥¼ GShard auxiliary lossì— í¬í•¨í•˜ì˜€ë‹¤.

- Auxiliary loss: $L_{aux} = \alpha_{aux} \cdot \frac{1}{E} \sum^E_{i=1} f_i \cdot m_i$

  - $\alpha_{aux}$: auxiliary lossì˜ ê³„ìˆ˜

  - $f_i$: ië²ˆì§¸ expertì— í• ë‹¹ëœ í† í° ë¹„ìœ¨

  - $m_i$: ië²ˆì§¸ expertì˜ í‰ê·  ë¼ìš°íŒ… í™•ë¥ 

- Global Router

  - í† í° ë¶„í¬ëŠ” ë‹¤ë¥¸ Expert Parallel(EP) ê·¸ë£¹ ì‚¬ì´ì—ì„œ ë‹¬ë¼ì§€ë¯€ë¡œ, load imbalancesë¥¼ ìœ ë°œí•œë‹¤. ë”°ë¼ì„œ, global token dispatching strategyë¥¼ EP ê·¸ë£¹ë“¤ ì‚¬ì´ì— ì ìš©

  - í† í°ì„ ë¶„ë°°í•˜ê¸° ì „ì— ê° expertê°€ ì²˜ë¦¬í•  ëŒ€ê¸° í† í° ìˆ˜ë¥¼ ë™ê¸°í™”í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ allgather í†µì‹  ë‹¨ê³„ë¥¼ ë„ì…

#### 2-2. Linear Attention

<div align="center"><img src="https://github.com/user-attachments/assets/6fbefa28-adff-4851-94cc-22014b8a0ccd" width="80%"></img></div>  

Linear Attentionì€ `right product kernel trick`ì„ ì‚¬ìš©í•´ linear complexityë¡œ ë³€í™˜í•œë‹¤.

ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ TransNormerì—ì„œì˜ ì˜ˆì‹œì™€ ê°™ì´ NormAttentionì€ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.

- $O = Norm((QK^T)V)$

  - $Q,K,V \in \mathcal{R}^{n X d}$ : ì¿¼ë¦¬, í‚¤, ë²¨ë¥˜ í–‰ë ¬

  - `n`: sequence length

  - `d` : feature dimension

- ìœ„ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë°”ë€” ìˆ˜ ìˆë‹¤: $O = Norm(Q(K^T V))$

  - ì‹œí€€ìŠ¤ ê¸¸ì´ì— ê´€ê³„ì—†ì´ $ğ‘‚(ğ‘‘Â²)$ì˜ ì¼ì •í•œ ê³„ì‚° ë³µì¡ë„ë¥¼ ë³´ì¥

  - ì´ëŠ” $K^T V$ í•­ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì „ì²´ ì–´í…ì…˜ í–‰ë ¬ì„ ë°˜ë³µ ê³„ì‚°í•  í•„ìš”ë¥¼ ì—†ì• ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§. ì´ì— ë¹„í•´, ì†Œí”„íŠ¸ë§¥ìŠ¤ ì–´í…ì…˜ì€ ì¶”ë¡  ì¤‘ì— $ğ‘‚(ğ‘›ğ‘‘Â²)$ì˜ ë³µì¡ë„ê°€ ë°œìƒë¨.

í•˜ì§€ë§Œ, `casual language modeling`ì—ì„œ right productì˜ íš¨ê³¼ê°€ ë–¨ì–´ì§€ê¸° ë•Œë¬¸ì—, `cumsum` ê³„ì‚°ì´ í•„ìš”í•œë° ì´ëŠ” ê³ íš¨ìœ¨ ë³‘ë ¬ ê³„ì‚°ì— ë°©í•´ëœë‹¤. ì´ì— ë”°ë¼, ìµœì‹  LLMë“¤ì— linear attentionì´ ì±„íƒë˜ì§€ ëª»í–ˆë‹¤.

#### 2-3. Lightning Attention

lightning attentionì€ linear attentionì˜ casual language modelingì—ì„œ ëŠë¦° `cumsum` ì—°ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ ìƒˆë¡­ê²Œ íƒ€ì¼ë§ ê¸°ë²•ì„ ì œì•ˆí–ˆë‹¤. 

ì´ ë°©ë²•ì˜ í•µì‹¬ì€ attention ê³„ì‚°ì„ intra-blcokê³¼ inter-blockì´ë¼ëŠ” 2ê°œì˜ ê°œë³„ ìš”ì†Œë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì— ìˆë‹¤. intra-block ì—°ì‚°ì—ì„œëŠ” left product attentionì´ ì ìš©ë˜ê³  inter-blockì—ì„œëŠ” right product attentionì´ ì ìš©ëœë‹¤. ì´ë ‡ê²Œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ì´ìœ ê°€ intra blockì€ í¬ê¸°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆì–´, ì „ì²´ ê³„ì‚° ë³µì¡ë„ëŠ” ì„ í˜•ìœ¼ë¡œ ìœ ì§€ì‹œí‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

- left product in casual attentionì€ ë‹¤ìŒì²˜ëŸ¼ ì •ì˜ëœë‹¤: $O = [(QK^T) âŠ™ M]V$ 

  - $M_{ts} = 1$ if $t >=s$, otherwise 0

- right productëŠ” ë‹¤ìŒê°™ì´ recursiveí•˜ê²Œ ê³„ì‚°ëœë‹¤: $kv_0 = 0, \ kv_t = kv_{t-1}+ k_tv_t^{\top}, \ o_t^{\top} = q_t^{\top}kv_t$

  - ì´ëŠ” ë‚´ì¬ì ìœ¼ë¡œ ë³‘ë ¬í™”ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, lightning attentionì€ íƒ€ì¼ë§ ê¸°ë²•ìœ¼ë¡œ attention scoreë¥¼ ê³„ì‚°í•œë‹¤.

- ì´ì— ë”°ë¼ Q,K,V í–‰ë ¬ì„ 2ê°œì˜ ê°œë³„ blockìœ¼ë¡œ ë‚˜ëˆˆë‹¤: 

  - <img src="https://github.com/user-attachments/assets/5fd9c7e9-1ad2-4a1b-8a96-b03e46f4657f" width="50%"></img>


  - right productë¥¼ í¼ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤: $kv_s = kv_0 + \sum^s_{j=1}k_jv_j^{\top}, \ s=1,..., m$ $o_s^{\top} = q_s^{\top} kv_s =q_s^{\top}kv_0 + q_s^{\top} \sum^s_{j=1} k_jv_j^{\top}$

  - ì´ë¥¼ block formìœ¼ë¡œ ë‹¤ì‹œ ì“°ë©´:  
  
    - <img src="https://github.com/user-attachments/assets/86c49bf6-792f-47ef-95dc-565f98c07c32" width="50%"></img>

      - intra-blockì¸ $[(Q_1K_1^{\top}) âŠ™M)]V_1$ ì€ left productë¥¼ ì‚¬ìš©í•˜ê³ , inter-blockì¸ $Q_1KV_0$ ì€ right productë¥¼ ì‚¬ìš©í•œë‹¤.

    - intra-blockì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ë‰  ìˆ˜ ìˆë‹¤:

      - <img src="https://github.com/user-attachments/assets/6d7cc153-d88b-4dbe-a82b-0ddf375e22ef" width="60%"></img>

  - 2ë²ˆì§¸ blockì„ ê³„ì‚°í•˜ê¸° ìœ„í•´, $KV_1 = kv_m$ ì„ ì‚¬ìš©í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤: $KV_1 = KV_0 + \sum^m_{j=1} k_m v_m^{\top} = KV_0 + K_1^{\top} V_1$

    - $KV_0 = kv_0$

í–‰ë ¬ì„ ì—¬ëŸ¬ ê°œì˜ blockë“¤ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì„ ë°˜ë³µì ìœ¼ë¡œ ì ìš©í•¨ìœ¼ë¡œì¨, ì‹¤ì œ ê³„ì‚° ë³µì¡ë„ë¥¼ ì„ í˜•ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. lightning attentionì˜ ê³„ì‚° ë³µì¡ë„ëŠ” $O(nd^2 + nBd)$ì´ë©°, ì—¬ê¸°ì„œ `B`ëŠ” block sizeì´ë‹¤.

![image](https://github.com/user-attachments/assets/4e7f7cb0-c942-4c44-8389-221fb352b805)


#### 2-4. Experiments & Results

ì‹¤í—˜ì„ ìœ„í•´ softmax(Flash attention-2 ì ìš©), lightning attention, hybrid-lightning attention modelì„ ë‹¤ì–‘í•œ í¬ê¸°(70M ~ 7B)ë¡œ í•™ìŠµí•˜ì˜€ë‹¤. ê° ëª¨ë¸ì€ 300B í† í°ì„ ì´ìš©í•´ 8192ì˜ context lengthë¡œ í•™ìŠµí•˜ì˜€ìœ¼ë©°, `Chinchilla`ì˜ í•™ìŠµ ë°©ë²•ì„ ë”°ëë‹¤.   
ì—¬ê¸°ì„œ, hybrid-lightning attention modelì€ ë§¤ 8ë²ˆì§¸ layerì— lightning attentionì´ ì•„ë‹Œ softmax attentionì„ ë‘” ëª¨ë¸ì´ë‹¤.

<div align="center"><img src="https://github.com/user-attachments/assets/d1a535f1-91ec-4e2c-82e8-4a0eb254a4d8" width="80%"></img></div>  


ì‹¤í—˜ê²°ê³¼, `NIAH`ë¥¼ ì œì™¸í•˜ë©´ lightning attentionì€ ê¸°ì¡´ transformer modelê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. í•˜ì§€ë§Œ, retrieval taskì—ì„œëŠ” ì•½í•œ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ë°, ì´ëŠ” hybrid modelì—ì„œëŠ” ì˜¤íˆë ¤ transformer modelì„ ëŠ¥ê°€í•˜ì˜€ë‹¤.   
ë”°ë¼ì„œ, hybrid-lightning attention modelì€ LLMì˜ in-context learningì— ì í•©í•˜ë‹¤ ë³¼ ìˆ˜ ìˆë‹¤.

<div align="center"><img src="https://github.com/user-attachments/assets/591732be-d2ca-4a62-9ad9-415f8be5a3ac" width="60%"></img></div>  


3B ëª¨ë¸ë“¤ì˜ end-to-end training speedë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì´ˆë‹¹ GPU í† í° ì²˜ë¦¬ìœ¨(TGS)ë¥¼ í™•ì¸í•œ ê²°ê³¼, lightning attentionì€ sequence lengthì™€ ìƒê´€ì—†ì´ ë³€í™”ì—†ëŠ” training speedë¥¼ ë³´ì˜€ìœ¼ë©° FlashAttention-2ë¥¼ ëŠ¥ê°€í•˜ëŠ” ìœ ì¼í•œ ì„ í˜• ëª¨ë¸ì´ë‹¤.

<div align="center"><img src="https://github.com/user-attachments/assets/ce01b169-39e0-4cbc-9944-05c037e025a1" width="80%"></img></div>  



- Module Ablations in MoE

  - Hybrid-lightning Attention VS Softmax Attention: 28B ëª¨ë¸(MoE with 5B)ì¸ softmax attentionìœ¼ë¡œ êµ¬ì„±ëœ base modelê³¼ ì—¬ê¸°ì—ì„œ ë§¤ 8ë²ˆì§¸ë§Œ softmax attentionìœ¼ë¡œ ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” lightning attentionìœ¼ë¡œ ë‘” hybrid-lightning attentionì„ ë¹„êµí–ˆë‹¤. ê·¸ ê²°ê³¼, lightning attentionì´ ëŒ€ë¶€ë¶„ì˜ benchmarkì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

  - Pre Layer Normalization VS Post Layer Normalization: PostNormì€ gradient ì†Œì‹¤ ë° í­ë°œ ë¬¸ì œë¡œ ì¸í•´ ê¸°ì¡´ transformer LLMì—ì„œëŠ” PreNormì´ ì‚¬ìš©ë˜ëŠ” ì¶”ì„¸ì˜€ë‹¤. í•˜ì§€ë§Œ ì‹¤í—˜ ê²°ê³¼ hybrid-lightning attention modelì—ì„œëŠ” PostNormì´ ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ë‹¤(ì—¬ê¸°ì—ì„œëŠ” DeepNorm ì‚¬ìš©).

<br></br>

### 3. Pre-Training

- Tokenizer: Byte-level BPE

  - resulting vocabulary size is set to 200K

- Data:  academic literature, books, web content, programming code

  - ë°ì´í„° ì§ˆì„ ë†’ì´ê¸° ìœ„í•´ í•„í„°ë§ ê³¼ì •ì„ ê±°ì¹¨(rule-based cleaning, deduplication)

  - ì—¬ëŸ¬ ë°ì´í„° ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ìƒ˜í”Œë§ ì‚¬ìš©


### 4. Post-Training

- Supervised Fine-Tuning (SFT)

  - `rejection sampling`ì„ ì´ìš©í•´ ë†’ì€ í’ˆì§ˆì˜ ëŒ€ë‹µ ìƒ

- Offline and Online Reinforcement Learning (RL)

  - Offline: DPOë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”

  - Online: Group Relative Policy Optimization(GRPO) ë¥¼ ë°”ê¾¸ì–´ ì‚¬ìš©

### 5. Results

<div align="center"><img src="https://github.com/user-attachments/assets/34436d00-d3a9-4c44-9e62-e71337ff3fda" width="60%"></img></div> 


<div align="center"><img src="https://github.com/user-attachments/assets/2dad2002-360d-4c5c-b42f-8e18699c932f" width="60%"></img></div>     
