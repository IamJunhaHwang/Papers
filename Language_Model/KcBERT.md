KcBERT: 한국어 댓글로 학습한 BERT
----------------------------------

- 저자: 이준범
- [KcBERT한국어댓글로학습한BERT.pdf](/uploads/f0df8968be4aa18e245374c9e4cc60ba/KcBERT한국어댓글로학습한BERT.pdf)
- 제32회 한글 및 한국어 정보처리 학술대회 논문집 (2020년)

서론
-------------

- 레이블 되지 않은 말뭉치로 사전 학습을 거친 모델로 전이 학습하여 높은 성능 성취
- 데이터셋에 종속적인 특징이 있는데 사용된 데이터셋은 한국어의 비중이 낮아 한국어에는 적합하지 않음
- 대부분의 한국어 BERT 모델들은 대부분 문어체 데이터로 학습한 모델임
- 구어체, 오타, 신조어에 대응하기 좋게 한국어 댓글 데이터로 학습한 KcBERT모델 개발함

KcBERT 모델 학습
----------------

- 데이터셋
  - 네이버 뉴스 댓글 데이터(2019.01.01 ~ 2020.06.11)
  - 1억5천만 댓글, 텍스트 기준 15GB

- 텍스트 정제
  - 한글 및 영어, 특수문자, 유니코드 이모지만 남김
  - 중복 문자열 축약('ㅋㅋㅋㅋ'->'ㅋㅋ')
  - 영어 대소문자 유지
  - 10글자 이하 댓글 제거
  - 중복 문장 제거
  - 정제 후 8천9백만 댓글, 텍스트 기준 12GB

- WordPiece 토크나이저 학습
  - 허깅페이스의 Tokenizers 라이브러리로 BERT WordPiece 토크나이저 학습
  - vocab 수 3만개
  - 6000개는 Alphabet allow로 한국어 개별 문자를 많이 포함하게 하여 out of vocab이 발생하지 않도록 학습함

- 학습 설정 및 환경
  - Positional Embedding 최대 300토큰 길이 까지만 학습 가능
  - 뉴스 댓글 자체가 300글자 제한이 있음
  - 기본적으로 BERT 모델과 동일

  - Base 모델 학습 하이퍼 파라미터 설정
    - Train Batch size: 128
    - Learning rate: 2e-5
    - Validation Batch size: 64
    - Warm-up Steps: 10(높게 설정하면 초기 Loss 수렴이 보다 빠르게 진행됨)

  - Large 모델 학습 하이퍼 파라미터 설정
    - Batch size: 128
    - Learning rate: 5e-5
    - Validation Batch size: 64
    - Warm-up Steps: 300,000

   - 학습 결과 및 체크포인트
     - 1M 스텝 부근까지 학습한 후 Loss가 가장 낮은 모델 선택
     - 1M 스텝 이후에는 뚜렷한 Loss 감소가 일어나지 않음
     - Base: 990k step checkpoint, Large: 970k step checkpoint

한국어 Transformers 계열 모델 간 성능 비교
-------------------------------------------

![image](https://user-images.githubusercontent.com/49019184/233265121-3cdab32a-b929-48c6-9d3f-c5d0110bbf52.png)

- 네이버 영화 리뷰 코퍼스에서 높은 성능을 보였음, 특히 Large의 경우 가장 높음
- NSMC와 Question Pari, KorQuaD 데이터셋에서 KoBERT와 동등하거나 그 이상임
- PAWS 같은 일부 태스크에서는 낮은 성능을 보이기도 함
- 댓글만 학습했다는 점에 있어 학습한 지식이 상대적으로 부족할 수 있음

결론
--------

- 최초로 댓글만 학습 데이터로 사용한 BERT 사전 학습 진행
- 타 한국어 BERT 모델과 비등한 성능을 보이며, 댓글과 관련된 테스트 데이터셋에서는 높은 성능을 보였음
- 학습 데이터셋이 가진 다양성의 한계로 보다 보편적인 지식을 학습하지는 못해 일부 테스트에서는 낮은 성능을 보였음
